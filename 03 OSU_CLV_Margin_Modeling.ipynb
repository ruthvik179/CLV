{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group Exercise CLV - Group 6\n",
    "## Margin Modeling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import (RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor,\n",
    "RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, AdaBoostClassifier)\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modeling data created in the Data Prep pipeline. Change path in below cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('combined_df.pkl', 'rb') as file:\n",
    "    combined_df = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing variable stats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" summary stats on combined data \") \n",
    "\"\"\"print(combined_df.describe(include='all').transpose())\"\"\"\n",
    "# describe only the numeric variables:\n",
    "print(combined_df.describe(include = [np.number]).transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic exploratory analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" A boxplot to investigate the distribution of target variable by a categorical variable \"\"\"\n",
    "\n",
    "axes = combined_df.boxplot(column='margin_Y_pos_1', by='tier',figsize=(15,15),\n",
    "                   whis=[5,95])\n",
    "# given the extreme values/range for future margin, limiting the range of the y-axis:\n",
    "axes.set(ylim=(-2000, 10000))\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping variables we don't want included in modeling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Any categorical variable for which we did not create dummy indicators        \"\"\"\n",
    "\"\"\"    and other variables we don't want in the model, or which don't make sense \"\"\"\n",
    "\"\"\"    to be included in model training, we are dropping here:                   \"\"\"\n",
    "\n",
    "\"\"\" Note:  we also want to drop acct_ID, but we are going to do that after partioning \"\"\"\n",
    "\"\"\"        because we need acct_ID for partioned sets to for reference later          \"\"\"\n",
    "\n",
    "x_train_df = combined_df.drop(['Zip','DMA','Area_Cd','activation_date','event_date',\n",
    "                            'ST','tier','tier_prev','demog_ownrent','Region'\n",
    "                              ],axis=1)\n",
    "\n",
    "x_train_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step was not performed for churn modeling.\n",
    "We are going to eliminate the records associated with the \"churners\".\n",
    "\n",
    "\n",
    "Because of our approach to calculating future value, we want to predict the expect future margin *given* that the customer remains a customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" first we keep the churners, to score them later \"\"\"\n",
    "x_churners_df = x_train_df.query('churn_year_plus1_ind==1')\n",
    "\"\"\" then we strip our the churners from the data to be used for training and testing \"\"\"\n",
    "x_train_df = x_train_df.query('churn_year_plus1_ind==0')\n",
    "\n",
    "x_train_df.shape\n",
    "# we will score the churners later, but don't want them used for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing any boolean columns into int datatypes to prevent errors\n",
    "boolean_columns = list(x_train_df.dtypes[x_train_df.dtypes == 'bool'].index)\n",
    "x_train_df[boolean_columns] = x_train_df[boolean_columns].astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partitioning data into Training and Test sets, creating X vs Y sets, and creating dataframes with just account IDs to be used to rejoin data later.\n",
    "\n",
    "\n",
    "**Note:** for margin modeling, we have a different target variable than we did for churn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_df, x_test_df = train_test_split(x_train_df, test_size = 0.30)\n",
    "\n",
    "\"\"\" will need the acct_IDs to rejoin data later, so pulling out now before dropping from actual model training data \"\"\"\n",
    "\n",
    "train_acct_IDs_df = x_train_df[['acct_ID']]\n",
    "test_acct_IDs_df  = x_test_df[['acct_ID']]\n",
    "churners_acct_IDs_df = x_churners_df[['acct_ID']]\n",
    "\n",
    "\"\"\" the target (or dependent) variable is stored in it's own dataframe \"\"\"\n",
    "\n",
    "y_train_df = x_train_df[['log_margin_Y_pos_1']]\n",
    "y_test_df  = x_test_df[['log_margin_Y_pos_1']]\n",
    "y_churners_df  = x_churners_df[['log_margin_Y_pos_1']]\n",
    "\n",
    "\"\"\" and we want to drop acct_ID and the target variable from the dataframe of independent variables \"\"\"\n",
    "\"\"\" PLUS we want to drop the target (future timeframe) variables for churn modeling                \"\"\"\n",
    "\n",
    "x_train_df = x_train_df.drop(['acct_ID','churn_year_plus1_ind', 'log_margin_Y_pos_1', 'margin_Y_pos_1'\n",
    "                             ],axis=1)\n",
    "\n",
    "x_test_df = x_test_df.drop(['acct_ID','churn_year_plus1_ind', 'log_margin_Y_pos_1', 'margin_Y_pos_1'\n",
    "                             ],axis=1)\n",
    "\n",
    "x_churners_df = x_churners_df.drop(['acct_ID','churn_year_plus1_ind', 'log_margin_Y_pos_1', 'margin_Y_pos_1'\n",
    "                                   ],axis=1)\n",
    "\n",
    "x_train_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the SelectKBest and f_regression to select K parameters with highest f-values.\n",
    "We'll use this as a preliminary non-machine-learning approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" using the ravel simply to avoid warning in log \"\"\"\n",
    "y_train_array=np.ravel(y_train_df)\n",
    "y_test_array=np.ravel(y_test_df)\n",
    "\n",
    "# this line can be used to suppress any warnings caused by missing values:\n",
    "\"\"\"np.seterr(divide='ignore', invalid='ignore')\"\"\"\n",
    "\n",
    "model1_selects = SelectKBest(f_regression, k=20).fit(x_train_df, y_train_array)\n",
    "\n",
    "x_train_selected_df = x_train_df[x_train_df.columns[model1_selects.get_support()]]\n",
    "x_test_selected_df  = x_test_df[x_test_df.columns[model1_selects.get_support()]]\n",
    "\"\"\" note: we are not reducing the columns in the held-out churners dataframe, because this is not our final model \"\"\"\n",
    "\n",
    "x_train_selected_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introducing variable reduction that addresses multicolinearity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate VIF for all independent variables and drop variables with VIF > 10\n",
    "thresh=10\n",
    "\n",
    "X = x_train_selected_df\n",
    "\n",
    "dropped=True\n",
    "\n",
    "while dropped==True:\n",
    "    cols = X.columns\n",
    "    variables = np.arange(X.shape[1])\n",
    "    \n",
    "    while dropped:\n",
    "        dropped=False\n",
    "        c = X[cols[variables]].values\n",
    "        vif = [variance_inflation_factor(c, ix) for ix in np.arange(c.shape[1])]\n",
    "        maxloc = vif.index(max(vif))\n",
    "\n",
    "    # Drop variables with VIF values greater than 10    \n",
    "    if max(vif) > thresh: \n",
    "        print('dropping \\'' + X[cols[variables]].columns[maxloc] + '\\' at index: ' + str(maxloc))\n",
    "        variables = np.delete(variables, maxloc)\n",
    "        dropped=True\n",
    "        \n",
    "        print('Remaining Shape:')\n",
    "        print(X.shape)\n",
    "    else:\n",
    "        dropped=False\n",
    "    X = X.iloc[:, variables]\n",
    "\n",
    "x_train_selected_df = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" reducing the test set to have the same columns as the training set \"\"\"\n",
    "\"\"\" note: we are not reducing the columns in the held-out churners dataframe, because this is not our final model \"\"\"\n",
    "\n",
    "x_test_selected_df=x_test_selected_df[x_train_selected_df.columns]\n",
    "\n",
    "print('Remaining variables in test set:')\n",
    "print(x_test_selected_df.columns)\n",
    "        \n",
    "x_test_selected_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the statsmodels package to see \"friendlier\" looking Regression output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" for the statsmodels, need to add a constant to fit intercept \"\"\"\n",
    "\"\"\" the add_constant function won't add a constant if there's already a variable with variance=0 \"\"\"\n",
    "\"\"\"     unless the has_constant option is added \"\"\"\n",
    "\n",
    "x_train_selected_df = sm.add_constant(x_train_selected_df, has_constant='add')\n",
    "x_test_selected_df  = sm.add_constant(x_test_selected_df, has_constant='add')\n",
    "\n",
    "\"\"\" warning:  the OLS here is case-sensitive \"\"\"\n",
    "model = sm.OLS(y_train_df, x_train_selected_df)\n",
    "result = model.fit()\n",
    "print(\" \") \n",
    "print(\"OLS model with selected variables\")\n",
    "print(\" \") \n",
    "print(result.summary())\n",
    "\n",
    "\"\"\" predicted values using the model \"\"\"\n",
    "\"\"\" using numpy copy to store result as an array \"\"\"\n",
    "\"\"\" manually adding a column header \"\"\"\n",
    "predict_train = pd.DataFrame(np.copy(result.predict(x_train_selected_df)), columns=[\"P_log_margin_Y_pos_1\"])\n",
    "predict_test = pd.DataFrame(np.copy(result.predict(x_test_selected_df)), columns=[\"P_log_margin_Y_pos_1\"])\n",
    "\n",
    "print(\" \") \n",
    "print(\"MSE train:\", mean_squared_error(y_train_array, predict_train))\n",
    "print(\"MSE test:\", mean_squared_error(y_test_array, predict_test))\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.scatter(y_train_array, predict_train)\n",
    "plt.xlabel(\"log margin\")\n",
    "plt.ylabel(\"predicted log margin\")\n",
    "plt.title(\"actual v predicted\")\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "print(\" \") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now to try a more advanced Machine Learning algorithm to see if we can get a better model.\n",
    "**Note:** with steps below, significant variables are selected based on GB model, not using the more simplified f-test approach above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make function to evaluate fitted models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Gradient Boosting \"\"\"\n",
    "params = {'n_estimators': 70, 'min_samples_split': 2,  'max_depth': 5, 'learning_rate': 0.01, 'loss': 'squared_error'}\n",
    "\n",
    "model7 = GradientBoostingRegressor(**params)\n",
    "\n",
    "\"\"\" switch the order of X and Y for the fit to work with GradientBoosting \"\"\"\n",
    "\"\"\" also, note that we are starting with the full training data, and not the selected variables from above \"\"\"\n",
    "result = model7.fit(x_train_df, y_train_array)\n",
    "\n",
    "\"\"\" returns mean accuracy\"\"\"\n",
    "accuracy = model7.score(x_test_df, y_test_df)\n",
    "print(\"GB overall accuracy, for model with all variables:\", accuracy)\n",
    "\n",
    "\"\"\" predicted values using the model \"\"\"\n",
    "\"\"\" using numpy copy to store result as an array \"\"\"\n",
    "\"\"\" manually adding a column header \"\"\"\n",
    "predict_train = pd.DataFrame(np.copy(model7.predict(x_train_df)), columns=[\"P_log_margin_Y_pos_1\"])\n",
    "predict_test = pd.DataFrame(np.copy(model7.predict(x_test_df)), columns=[\"P_log_margin_Y_pos_1\"])\n",
    "\n",
    "print(\" \") \n",
    "print(\"MSE train, for model with all variables:\", mean_squared_error(y_train_array, predict_train))\n",
    "print(\"MSE test, for model with all variables:\", mean_squared_error(y_test_array, predict_test))\n",
    "\n",
    "# Plot feature importance\n",
    "feature_importance = result.feature_importances_\n",
    "# make importances relative to max importance\n",
    "feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "plt.figure(figsize=(50,50))\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.barh(pos, feature_importance[sorted_idx], align='center')\n",
    "plt.yticks(pos, x_train_df.columns[sorted_idx])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.title('Variable Importance, for model with all variables')\n",
    "plt.show()\n",
    "\n",
    "\"\"\" plotting MSE with each iteration for the model with all variables \"\"\"\n",
    "\n",
    "test_MSE  = np.zeros((params['n_estimators'],), dtype=np.float64)\n",
    "train_MSE = np.zeros((params['n_estimators'],), dtype=np.float64)\n",
    "\n",
    "for i, y_pred in enumerate(model7.staged_predict(x_test_df)):\n",
    "    test_MSE[i] = mean_squared_error(y_test_array, y_pred)\n",
    "\n",
    "for i, y_pred in enumerate(model7.staged_predict(x_train_df)):\n",
    "    train_MSE[i] = mean_squared_error(y_train_array, y_pred)\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('MSE by iteration, for the model with all variables')\n",
    "plt.plot(np.arange(params['n_estimators']) + 1, train_MSE, 'b-',\n",
    "         label='Training Set MSE')\n",
    "plt.plot(np.arange(params['n_estimators']) + 1, test_MSE, 'r-',\n",
    "         label='Test Set MSE')\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('Boosting Iterations')\n",
    "plt.ylabel('MSE')\n",
    "\n",
    "\"\"\" select only impactful features \"\"\"\n",
    "\"\"\" note:  decrease/increase this threshold (a relative number between 0 and 100) to select more/less features \"\"\"\n",
    "selected_features = (feature_importance > 10)\n",
    "\n",
    "\"\"\" creating a dataframe of independent variables, from the training (and test) data \"\"\"\n",
    "\"\"\"     having only those independent variables selected above \"\"\"\n",
    "\n",
    "x_train_selected_df = x_train_df[x_train_df.columns[selected_features]]\n",
    "x_test_selected_df = x_test_df[x_test_df.columns[selected_features]]\n",
    "\n",
    "\"\"\" we are also reducing columns in churners data, as this is our final model \"\"\"\n",
    "\"\"\"     and any data we want to score needs to have the correct columns \"\"\"\n",
    "x_churners_selected_df = x_churners_df[x_churners_df.columns[selected_features]]\n",
    "\n",
    "\"\"\" refitting the model using a subset of features \"\"\"\n",
    "\"\"\" had to switch the order of X and Y for the fit to work with GradientBoosting \"\"\"\n",
    "result = model7.fit(x_train_selected_df, y_train_array)\n",
    "\"\"\" returns mean accuracy\"\"\"\n",
    "accuracy = model7.score(x_test_selected_df, y_test_df)\n",
    "print(\"GB overall accuracy, for model with only selected features:\", accuracy)\n",
    "\n",
    "\"\"\" predicted values using the model \"\"\"\n",
    "\"\"\" using numpy copy to store result as an array \"\"\"\n",
    "\"\"\" manually adding a column header \"\"\"\n",
    "predict_train = pd.DataFrame(np.copy(model7.predict(x_train_selected_df)), columns=[\"P_log_margin_Y_pos_1\"])\n",
    "predict_test = pd.DataFrame(np.copy(model7.predict(x_test_selected_df)), columns=[\"P_log_margin_Y_pos_1\"])\n",
    "predict_churners = pd.DataFrame(np.copy(model7.predict(x_churners_selected_df)), columns=[\"P_log_margin_Y_pos_1\"])\n",
    "\n",
    "print(\" \") \n",
    "print(\"MSE train, for model with only selected features:\", mean_squared_error(y_train_array, predict_train))\n",
    "print(\"MSE test, for model with only selected features:\", mean_squared_error(y_test_array, predict_test))\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.scatter(y_train_array, predict_train)\n",
    "plt.xlabel(\"log margin\")\n",
    "plt.ylabel(\"predicted log margin\")\n",
    "plt.title(\"actual v predicted, for model using only selected variables\")\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "print(\" \") \n",
    "plt.show()\n",
    "\n",
    "\"\"\" plotting MSE with each iteration \"\"\"\n",
    "\n",
    "test_MSE  = np.zeros((params['n_estimators'],), dtype=np.float64)\n",
    "train_MSE = np.zeros((params['n_estimators'],), dtype=np.float64)\n",
    "\n",
    "for i, y_pred in enumerate(model7.staged_predict(x_test_selected_df)):\n",
    "    test_MSE[i] = mean_squared_error(y_test_array, y_pred)\n",
    "\n",
    "for i, y_pred in enumerate(model7.staged_predict(x_train_selected_df)):\n",
    "    train_MSE[i] = mean_squared_error(y_train_array, y_pred)\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('MSE by iteration, for model using only selected features')\n",
    "plt.plot(np.arange(params['n_estimators']) + 1, train_MSE, 'b-',\n",
    "         label='Training Set MSE')\n",
    "plt.plot(np.arange(params['n_estimators']) + 1, test_MSE, 'r-',\n",
    "         label='Test Set MSE')\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('Boosting Iterations')\n",
    "plt.ylabel('MSE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = model7.feature_importances_\n",
    "\"\"\" make importances relative to max importance \"\"\"\n",
    "feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.barh(pos, feature_importance[sorted_idx], align='center')\n",
    "plt.yticks(pos, x_train_selected_df.columns[sorted_idx])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.title('Variable Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appending the train and test sets, and keeping the acct_IDs and predicted churn probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" append together train and test predicted values, keeping only acct ID and predicted values \"\"\"\n",
    "\"\"\" also predicting values for the held-out churners and appending those, so we have all rows \"\"\"\n",
    "\n",
    "\"\"\" note that the index for the predicted values vector \"\"\"\n",
    "\"\"\" is reset, and thus complicates a join back with the original \"\"\"\n",
    "\"\"\" thus, reset the index on the original data, then join with predicted \"\"\"\n",
    "\"\"\" WARNING: you have to do the same, to join back with dataframe of regressors \"\"\"\n",
    "y_train_df = y_train_df.reset_index(drop=True)\n",
    "y_test_df = y_test_df.reset_index(drop=True)\n",
    "y_churners_df = y_churners_df.reset_index(drop=True)\n",
    "\n",
    "\"\"\" joining predicted and actuals \"\"\"\n",
    "\"\"\" NOTE:  for reference, for now...to be used more later to create complete \"\"\"\n",
    "\"\"\"     dataframe with all obs and inputs and outputs \"\"\"\n",
    "\n",
    "train_accounts = train_acct_IDs_df.reset_index(drop=True)\n",
    "pred_v_actual_train = y_train_df.join(predict_train)\n",
    "pred_v_actual_train = pred_v_actual_train.join(train_accounts)\n",
    "\n",
    "test_accounts = test_acct_IDs_df.reset_index(drop=True)\n",
    "pred_v_actual_test = y_test_df.join(predict_test)\n",
    "pred_v_actual_test = pred_v_actual_test.join(test_accounts)\n",
    "\n",
    "churners_accounts = churners_acct_IDs_df.reset_index(drop=True)\n",
    "pred_v_actual_churners = y_churners_df.join(predict_churners)\n",
    "pred_v_actual_churners = pred_v_actual_churners.join(churners_accounts)\n",
    "\n",
    "\"\"\" appending test to train, and calling it train \"\"\"\n",
    "pred_v_actual_train = pd.concat([pred_v_actual_train, pred_v_actual_test], ignore_index=True)\n",
    "\n",
    "\"\"\" appending churners to train, and calling it train \"\"\"\n",
    "pred_v_actual_train = pd.concat([pred_v_actual_train, pred_v_actual_churners], ignore_index=True)\n",
    "\n",
    "\"\"\" dropping the actuals and keeping just the predicted values \"\"\"\n",
    "pred_v_actual_train = pred_v_actual_train.drop(['log_margin_Y_pos_1'\n",
    "                             ],axis=1)\n",
    "\n",
    "\"\"\" and then save predicted values by acct ID to csv \"\"\"\n",
    "\n",
    "## Change path in below code\n",
    "pred_v_actual_train.to_csv('margin_scores.csv',index=False)\n",
    "pred_v_actual_train.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
