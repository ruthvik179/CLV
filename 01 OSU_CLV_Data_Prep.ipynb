{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group Exercise CLV - Group 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install scikit-learn\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read In Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_df = pd.read_csv('data/TP_churn_data.csv')\n",
    "margin_df = pd.read_csv('data/TP_margin_data.csv')\n",
    "demog_df = pd.read_csv('data/demog_pmm_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modify margin data to only have ANNUAL margin numbers,\n",
    "Print stats on the new margin dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "margin_df['rev_Y_neg_1']=margin_df['trans_amt_t_minus_00']+margin_df['trans_amt_t_minus_01']+margin_df['trans_amt_t_minus_02']+margin_df['trans_amt_t_minus_03']+margin_df['trans_amt_t_minus_04']+margin_df['trans_amt_t_minus_05']+margin_df['trans_amt_t_minus_06']+margin_df['trans_amt_t_minus_07']+margin_df['trans_amt_t_minus_08']+margin_df['trans_amt_t_minus_09']+margin_df['trans_amt_t_minus_10']+margin_df['trans_amt_t_minus_11']\n",
    "margin_df['cost_Y_neg_1']=margin_df['cost_amt_t_minus_00']+margin_df['cost_amt_t_minus_01']+margin_df['cost_amt_t_minus_02']+margin_df['cost_amt_t_minus_03']+margin_df['cost_amt_t_minus_04']+margin_df['cost_amt_t_minus_05']+margin_df['cost_amt_t_minus_06']+margin_df['cost_amt_t_minus_07']+margin_df['cost_amt_t_minus_08']+margin_df['cost_amt_t_minus_09']+margin_df['cost_amt_t_minus_10']+margin_df['cost_amt_t_minus_11']\n",
    "margin_df['margin_Y_neg_1']=margin_df['rev_Y_neg_1']-margin_df['cost_Y_neg_1']\n",
    "\n",
    "margin_df['rev_Y_neg_2']=margin_df['trans_amt_t_minus_12']+margin_df['trans_amt_t_minus_13']+margin_df['trans_amt_t_minus_14']+margin_df['trans_amt_t_minus_15']+margin_df['trans_amt_t_minus_16']+margin_df['trans_amt_t_minus_17']+margin_df['trans_amt_t_minus_18']+margin_df['trans_amt_t_minus_19']+margin_df['trans_amt_t_minus_20']+margin_df['trans_amt_t_minus_21']+margin_df['trans_amt_t_minus_22']+margin_df['trans_amt_t_minus_23']\n",
    "margin_df['cost_Y_neg_2']=margin_df['cost_amt_t_minus_12']+margin_df['cost_amt_t_minus_13']+margin_df['cost_amt_t_minus_14']+margin_df['cost_amt_t_minus_15']+margin_df['cost_amt_t_minus_16']+margin_df['cost_amt_t_minus_17']+margin_df['cost_amt_t_minus_18']+margin_df['cost_amt_t_minus_19']+margin_df['cost_amt_t_minus_20']+margin_df['cost_amt_t_minus_21']+margin_df['cost_amt_t_minus_22']+margin_df['cost_amt_t_minus_23']\n",
    "margin_df['margin_Y_neg_2']=margin_df['rev_Y_neg_2']-margin_df['cost_Y_neg_2']\n",
    "\n",
    "margin_df['rev_Y_neg_3']=margin_df['trans_amt_t_minus_24']+margin_df['trans_amt_t_minus_25']+margin_df['trans_amt_t_minus_26']+margin_df['trans_amt_t_minus_27']+margin_df['trans_amt_t_minus_28']+margin_df['trans_amt_t_minus_29']+margin_df['trans_amt_t_minus_30']+margin_df['trans_amt_t_minus_31']+margin_df['trans_amt_t_minus_32']+margin_df['trans_amt_t_minus_33']+margin_df['trans_amt_t_minus_34']+margin_df['trans_amt_t_minus_35']\n",
    "margin_df['cost_Y_neg_3']=margin_df['cost_amt_t_minus_24']+margin_df['cost_amt_t_minus_25']+margin_df['cost_amt_t_minus_26']+margin_df['cost_amt_t_minus_27']+margin_df['cost_amt_t_minus_28']+margin_df['cost_amt_t_minus_29']+margin_df['cost_amt_t_minus_30']+margin_df['cost_amt_t_minus_31']+margin_df['cost_amt_t_minus_32']+margin_df['cost_amt_t_minus_33']+margin_df['cost_amt_t_minus_34']+margin_df['cost_amt_t_minus_35']\n",
    "margin_df['margin_Y_neg_3']=margin_df['rev_Y_neg_3']-margin_df['cost_Y_neg_3']\n",
    "\n",
    "margin_df['rev_Y_pos_1']=margin_df['trans_amt_t_plus_01']+margin_df['trans_amt_t_plus_02']+margin_df['trans_amt_t_plus_03']+margin_df['trans_amt_t_plus_04']+margin_df['trans_amt_t_plus_05']+margin_df['trans_amt_t_plus_06']+margin_df['trans_amt_t_plus_07']+margin_df['trans_amt_t_plus_08']+margin_df['trans_amt_t_plus_09']+margin_df['trans_amt_t_plus_10']+margin_df['trans_amt_t_plus_11']+margin_df['trans_amt_t_plus_12']\n",
    "margin_df['cost_Y_pos_1']=margin_df['cost_amt_t_plus_01']+margin_df['cost_amt_t_plus_02']+margin_df['cost_amt_t_plus_03']+margin_df['cost_amt_t_plus_04']+margin_df['cost_amt_t_plus_05']+margin_df['cost_amt_t_plus_06']+margin_df['cost_amt_t_plus_07']+margin_df['cost_amt_t_plus_08']+margin_df['cost_amt_t_plus_09']+margin_df['cost_amt_t_plus_10']+margin_df['cost_amt_t_plus_11']+margin_df['cost_amt_t_plus_12']\n",
    "margin_df['margin_Y_pos_1']=margin_df['rev_Y_pos_1']-margin_df['cost_Y_pos_1']\n",
    "\n",
    "margin_df = margin_df[['acct_ID','margin_Y_neg_1','margin_Y_neg_2','margin_Y_neg_3','margin_Y_pos_1']]\n",
    "\n",
    "print(\" summary stats on margin data \") \n",
    "print(margin_df.describe(include='all').transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create bar chart to explore churn rates by tiers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = churn_df.groupby(['tier'])['churn_year_plus1_ind'].mean()\n",
    "temp_df.plot.bar(title=\"Churn Rate by Tiers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create histogram to explore distribution of variables in churn data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_df['purch_amt_life'].plot.hist(title=\"Distribution of Purchase Amount\",bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create histogram to explore distribution of variables in margin data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = margin_df.query('-1000<margin_Y_neg_1<3000')\n",
    "temp_df['margin_Y_neg_1'].plot.hist(title=\"Distribution of Prior Year Margin\",bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering on the Churn data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" when avg interval = -1 replace with 36 months \"\"\"\n",
    "churn_df['purch_avg_interval'].replace([-1], 36, inplace=True)\n",
    "\n",
    "\"\"\" when avg purchase = -1 replace with 0 \"\"\"\n",
    "churn_df['purch_avg_36mo'].replace([-1], 0, inplace=True)\n",
    "\n",
    "churn_df['log_order_cnt_36mo']=np.log10(churn_df['order_cnt_36mo']+1)\n",
    "churn_df['log_purch_amt_36mo']=np.log10(churn_df['purch_amt_36mo']+1)\n",
    "churn_df['log_purch_amt_life']=np.log10(churn_df['purch_amt_life']+1)\n",
    "churn_df['log_purch_avg_36mo']=np.log10(churn_df['purch_avg_36mo']+1)\n",
    "churn_df['log_purch_cnt_life']=np.log10(churn_df['purch_cnt_life']+1)\n",
    "churn_df['log_purch_cnt_tt_36mo']=np.log10(churn_df['purch_cnt_tt_36mo']+1)\n",
    "churn_df['log_resp_cnt_36mo']=np.log10(churn_df['resp_cnt_36mo']+1)\n",
    "\n",
    "churn_df['perc_purch_cc_36mo']=np.where(churn_df['purch_cnt_tt_36mo']==0,0,churn_df['purch_cnt_cc_36mo']/churn_df['purch_cnt_tt_36mo'])\n",
    "churn_df['perc_purch_ck_36mo']=np.where(churn_df['purch_cnt_tt_36mo']==0,0,churn_df['purch_cnt_ck_36mo']/churn_df['purch_cnt_tt_36mo'])\n",
    "churn_df['perc_purch_et_36mo']=np.where(churn_df['purch_cnt_tt_36mo']==0,0,churn_df['purch_cnt_et_36mo']/churn_df['purch_cnt_tt_36mo'])\n",
    "\n",
    "churn_df = churn_df.drop(['order_cnt_36mo','purch_amt_36mo','purch_amt_life','purch_avg_36mo','purch_cnt_life',\n",
    "                          'purch_cnt_tt_36mo','resp_cnt_36mo',\n",
    "                          'purch_cnt_cc_36mo','purch_cnt_ck_36mo','purch_cnt_et_36mo'],axis=1)\n",
    "\n",
    "print(\" summary stats on churn data \") \n",
    "print(churn_df.describe(include='all').transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Engineering On State Codes\n",
    "- Drop original high cardinality column in favor of regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping dictionary for all 50 states to regions\n",
    "state_to_region = {\n",
    "    'AL': 'South', 'AK': 'Alaska', 'AZ': 'Southwest', 'AR': 'South', 'CA': 'West',\n",
    "    'CO': 'West', 'CT': 'Northeast', 'DE': 'Northeast', 'FL': 'South', 'GA': 'South',\n",
    "    'HI': 'Hawaii', 'ID': 'West', 'IL': 'Midwest', 'IN': 'Midwest', 'IA': 'Midwest',\n",
    "    'KS': 'Midwest', 'KY': 'Southeast', 'LA': 'South', 'ME': 'Northeast', 'MD': 'Northeast',\n",
    "    'MA': 'Northeast', 'MI': 'Midwest', 'MN': 'Midwest', 'MS': 'South', 'MO': 'Midwest',\n",
    "    'MT': 'Midwest', 'NE': 'Midwest', 'NV': 'Southwest', 'NH': 'Northeast', 'NJ': 'Northeast',\n",
    "    'NM': 'Southwest', 'NY': 'Northeast', 'NC': 'Southeast', 'ND': 'Midwest', 'OH': 'Northeast',\n",
    "    'OK': 'South', 'OR': 'West', 'PA': 'Northeast', 'RI': 'Northeast', 'SC': 'Southeast',\n",
    "    'SD': 'Midwest', 'TN': 'Southeast', 'TX': 'South', 'UT': 'West', 'VT': 'Northeast',\n",
    "    'VA': 'Southeast', 'WA': 'West', 'WV': 'Southeast', 'WI': 'Midwest', 'WY': 'Midwest'\n",
    "}\n",
    "\n",
    "# Create a new 'Region' column based on the 'ST' column\n",
    "churn_df['Region'] = churn_df['ST'].map(state_to_region)\n",
    "\n",
    "# Fill NaN's\n",
    "churn_df['Region'] = churn_df['Region'].fillna('Other')\n",
    "\n",
    "# Drop original column\n",
    "combined_df.drop('ST', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## histogram plot on new log variable in Churn data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_df['log_purch_amt_life'].plot.hist(title=\"Distribution of LOG of Purchase Amount\",bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering on margin data, while keeping original variables.\n",
    "## Using logs with an offset that will account for very large negative margins:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "margin_df['margin_3yr_avg']=(margin_df['margin_Y_neg_1']+margin_df['margin_Y_neg_2']+margin_df['margin_Y_neg_3'])/3\n",
    "margin_df['log_margin_Y_neg_1']=np.log10(margin_df['margin_Y_neg_1']+50000)\n",
    "margin_df['log_margin_Y_neg_2']=np.log10(margin_df['margin_Y_neg_2']+50000)\n",
    "margin_df['log_margin_Y_neg_3']=np.log10(margin_df['margin_Y_neg_3']+50000)\n",
    "margin_df['log_margin_3yr_avg']=np.log10(margin_df['margin_3yr_avg']+50000)\n",
    "margin_df['log_margin_Y_pos_1']=np.log10(margin_df['margin_Y_pos_1']+50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histogram on transformed target for Margin data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "margin_df['log_margin_Y_pos_1'].plot.hist(title=\"Distribution of LOG of Future Margin\",bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatter plot on Future Margin vs Prior Margin (logarithms):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(margin_df['log_margin_3yr_avg'], margin_df['log_margin_Y_pos_1'])\n",
    "plt.xlabel(\"3-Year Average Margin (log)\")\n",
    "plt.ylabel(\"Future Year Margin (log)\")\n",
    "plt.title(\"Scatter Plot on Margin Data\")\n",
    "print(\" \") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eliminating duplicate acct_IDs before joining data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_df.drop_duplicates(subset=['acct_ID'], keep='first', inplace=True)\n",
    "churn_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "margin_df.drop_duplicates(subset=['acct_ID'], keep='first', inplace=True)\n",
    "margin_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demog_df.drop_duplicates(subset=['acct_ID'], keep='first', inplace=True)\n",
    "demog_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join Churn and Margin data with Demographics by acct_ID:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.merge(churn_df, margin_df, on='acct_ID', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.merge(combined_df, demog_df, on='acct_ID', how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demographics Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform **demog_LOR** to numerical **LOR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom function to extract numeric part\n",
    "def extract_numeric(value):\n",
    "    if value == '20 or more Years':\n",
    "        return int(21*12+6)\n",
    "    elif 'Year' in value:\n",
    "        return int(re.search(r'\\d+', value).group())*12+6\n",
    "    elif 'Months' in value:\n",
    "        return int(re.search(r'\\d+', value).group())+3\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Apply the custom function to the column\n",
    "combined_df['LOR'] = combined_df['demog_LOR'].apply(extract_numeric)\n",
    "\n",
    "# Drop original column\n",
    "combined_df.drop('demog_LOR', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform **demog_assets** to numerical **Assets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom function to extract numeric part\n",
    "def extract_numeric_assets(value):\n",
    "    if value == 'IPA <= 25000':\n",
    "        return int(1)\n",
    "    else:\n",
    "        return int(re.search(r'\\d+', value).group())\n",
    "\n",
    "# Apply the custom function to the column\n",
    "combined_df['Assets'] = combined_df['demog_assets'].apply(extract_numeric_assets)\n",
    "\n",
    "# Drop original column\n",
    "combined_df.drop('demog_assets', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform **demog_inc_fine** to numerical **Inc_Fine**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom function to extract numeric part\n",
    "def extract_numeric_assets(value):\n",
    "    if value == 'Less than $15000':\n",
    "        return int(1)\n",
    "    else:\n",
    "        return int(re.search(r'\\d+', value).group())\n",
    "\n",
    "# Apply the custom function to the column\n",
    "combined_df['Inc_Fine'] = combined_df['demog_inc_fine'].apply(extract_numeric_assets)\n",
    "\n",
    "# Drop original column\n",
    "combined_df.drop('demog_inc_fine', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform **demog_inc_crs** to numerical **Inc_Crs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom function to extract numeric part\n",
    "def extract_numeric_assets(value):\n",
    "    if value == 'Less than $15000':\n",
    "        return int(1)\n",
    "    else:\n",
    "        return int(re.search(r'\\d+', value).group())\n",
    "\n",
    "# Apply the custom function to the column\n",
    "combined_df['Inc_Crs'] = combined_df['demog_inc_crs'].apply(extract_numeric_assets)\n",
    "\n",
    "# Drop original column\n",
    "combined_df.drop('demog_inc_crs', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform **demog_age** to numeric **Age**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace specific values\n",
    "combined_df['Age'] = combined_df['demog_age'].replace({'18-24 years old': 20, '25-34 years old': 30, '35-44 years old': 40, '45-54 years old': 50, '55-64 years old': 60, '65-74 years old': 70, '75+ years old': 80})\n",
    "\n",
    "# Drop original column\n",
    "combined_df.drop('demog_age', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform **demog_homevalue** to numerical **Home_Value**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom function to extract numeric part\n",
    "def extract_numeric_assets(value):\n",
    "    if value == 'Less than $50000':\n",
    "        return int(1)\n",
    "    else:\n",
    "        return int(re.search(r'\\d+', value).group())\n",
    "\n",
    "# Apply the custom function to the column\n",
    "combined_df['Home_Value'] = combined_df['demog_homevalue'].apply(extract_numeric_assets)\n",
    "\n",
    "# Drop original column\n",
    "combined_df.drop('demog_homevalue', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of Demographics Categorical Columns to Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "additions_to_OHC = ['demog_ownrent']\n",
    "\n",
    "original_OHC_list = ['ST', 'tier', 'tier_prev']\n",
    "\n",
    "full_OHC_list = original_OHC_list + additions_to_OHC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dummy variables for certain categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" WARNING: this commented-out approach would create dummy indicators for all \"\"\"\n",
    "\"\"\"    categorical variables, including those with very high cardinality, such as zip codes \"\"\"\n",
    "\"\"\"    and also would drop the original categorical variables and only keep the dummy indicators \"\"\"\n",
    "\n",
    "\"\"\" combined_df = pd.get_dummies(combined_df) \"\"\"\n",
    "\n",
    "\"\"\" this approach hand-picks only certain variables for which to create dummy indicators \"\"\"\n",
    "\"\"\" and keeps the original variables too \"\"\"\n",
    "\n",
    "for column in full_OHC_list:\n",
    "    dummies = pd.get_dummies(combined_df[column],prefix=column)\n",
    "    combined_df[dummies.columns] = dummies\n",
    "\n",
    "print(\" \") \n",
    "print(\"New dataset with dummies\") \n",
    "print(\" \") \n",
    "print(combined_df.head(10)) \n",
    "print(\" \") \n",
    "print(\"Columns in new data\")\n",
    "print(\" \") \n",
    "print(combined_df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export updated data as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('combined_df.pkl', 'wb') as file:\n",
    "    pickle.dump(combined_df, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
