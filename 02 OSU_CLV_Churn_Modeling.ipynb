{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group Exercise CLV - Group 6\n",
    "## Churn Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turns off warnings in the jupyter notebook\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing some standard packages, as well as packages needed for modeling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import (RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor,\n",
    "RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, AdaBoostClassifier)\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import re\n",
    "import pickle\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import modeling data created in the Data Prep pipeline. Change the path in below cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('combined_df.pkl', 'rb') as file:\n",
    "    combined_df = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doing some basic exploratory analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Analyzing average values of numeric variables, by the levels of the binary target variable \"\"\"\n",
    "print(\" \") \n",
    "print(\"averages by churn ind\")\n",
    "print(\" \") \n",
    "print(combined_df.groupby('churn_year_plus1_ind')['log_purch_amt_life'].mean())\n",
    "\n",
    "print(\" \") \n",
    "print(\" \") \n",
    "print(combined_df.groupby('churn_year_plus1_ind')['scheduled_purchase_flg'].mean())\n",
    "\n",
    "print(\" \") \n",
    "print(\" \") \n",
    "print(combined_df.groupby('churn_year_plus1_ind')['months_since_last_trans'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" A boxplot to investigate the distribution of numeric variables by the levels of the target \"\"\"\n",
    "\n",
    "combined_df.boxplot(column='log_order_cnt_36mo', by='churn_year_plus1_ind',figsize=(15,15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing variable stats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" summary stats on combined data \") \n",
    "print(combined_df.describe(include='all').transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping variables we don't want included in modeling, and creating new version of data for modeling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Any categorical variable for which we did not create dummy indicators        \"\"\"\n",
    "\"\"\"    and other variables we don't want in the model, or which don't make sense \"\"\"\n",
    "\"\"\"    to be included in model training, we are dropping here:                   \"\"\"\n",
    "\n",
    "\"\"\" Note:  we also want to drop acct_ID, but we are going to do that after partioning \"\"\"\n",
    "\"\"\"        because we need acct_ID for partioned sets to for reference later          \"\"\"\n",
    "\n",
    "x_train_df = combined_df.drop(['Zip','DMA','Area_Cd','activation_date','event_date',\n",
    "                            'ST','tier','tier_prev','demog_ownrent','Region'\n",
    "                              ],axis=1)\n",
    "\n",
    "x_train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing any boolean columns into int datatypes to prevent errors\n",
    "boolean_columns = list(x_train_df.dtypes[x_train_df.dtypes == 'bool'].index)\n",
    "x_train_df[boolean_columns] = x_train_df[boolean_columns].astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partitioning data into Training and Test sets, creating X vs Y sets, and\n",
    "##     creating dataframes with just account IDs to be used to rejoin data later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_df, x_test_df = train_test_split(x_train_df, test_size = 0.30)\n",
    "\n",
    "\"\"\" will need the acct_IDs to rejoin data later, so pulling out now before dropping from actual model training data \"\"\"\n",
    "\n",
    "train_acct_IDs_df = x_train_df[['acct_ID']]\n",
    "test_acct_IDs_df  = x_test_df[['acct_ID']]\n",
    "\n",
    "\"\"\" the target (or dependent) variable is stored in it's own dataframe \"\"\"\n",
    "\n",
    "y_train_df = x_train_df[['churn_year_plus1_ind']]\n",
    "y_test_df  = x_test_df[['churn_year_plus1_ind']]\n",
    "\n",
    "\"\"\" and we want to drop acct_ID and the target variable from the dataframe of independent variables \"\"\"\n",
    "\"\"\" PLUS we want to drop the target (future timeframe) variables for margin modeling                \"\"\"\n",
    "\n",
    "x_train_df = x_train_df.drop(['acct_ID','churn_year_plus1_ind', 'log_margin_Y_pos_1', 'margin_Y_pos_1'\n",
    "                             ],axis=1)\n",
    "\n",
    "x_test_df = x_test_df.drop(['acct_ID','churn_year_plus1_ind', 'log_margin_Y_pos_1', 'margin_Y_pos_1'\n",
    "                             ],axis=1)\n",
    "\n",
    "x_train_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting with a scikit_learn Logistic Regression Model\n",
    "##    and using that modeling algorithm to select variables.\n",
    "\n",
    "## Note:  change the C parameter and rerun to get reasonable number of selected vars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" models penalized with the L1 norm have sparse solutions: many of their estimated coefficients are zero \"\"\"\n",
    "\"\"\" SelectFromModel can be used to to select effects with non-zero coefficients \"\"\"\n",
    "\"\"\" the parameter C controls the sparsity: the smaller C the fewer features selected \"\"\"\n",
    "\n",
    "\"\"\" using the ravel simply to avoid warning in log \"\"\"\n",
    "y_train_array=np.ravel(y_train_df)\n",
    "\n",
    "\"\"\" start with C = 1.0, then try 0.1, etc \"\"\"\n",
    "model1 = linear_model.LogisticRegression(C=0.01, solver='liblinear', penalty='l1', tol=0.01)\n",
    "model1_fit = model1.fit(x_train_df, y_train_array)\n",
    "model1_selects = SelectFromModel(model1_fit, prefit=True)\n",
    "\"\"\"x_train_selected_df = model1_selects.transform(x_train_df)\"\"\"\n",
    "\n",
    "\"\"\" in lieu of commented-out transform method above, can use this method to explicitly define columns to keep \"\"\"\n",
    "x_train_selected_df = x_train_df[x_train_df.columns[model1_selects.get_support()]]\n",
    "x_test_selected_df  = x_test_df[x_test_df.columns[model1_selects.get_support()]]\n",
    "\n",
    "coefficients = pd.DataFrame(model1_fit.coef_)\n",
    "coefficients = np.transpose(coefficients[coefficients.columns[model1_selects.get_support()]])\n",
    "coefficients = coefficients.reset_index(drop=True)\n",
    "\n",
    "coefficients_match = pd.concat([pd.DataFrame(x_train_selected_df.columns),coefficients], axis = 1)\n",
    "\n",
    "coefficients.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" \") \n",
    "print(\"Coefficients for selected variables\")\n",
    "coefficients_match.head(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Need to introduce variable reduction that addresses multicolinearity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate VIF for all independent variables and drop variables with VIF > 10\n",
    "\n",
    "def calculate_vif_(X, thresh=10):\n",
    "    cols = X.columns\n",
    "    variables = np.arange(X.shape[1])\n",
    "    dropped=True\n",
    "    while dropped:\n",
    "        dropped=False\n",
    "        c = X[cols[variables]].values\n",
    "        vif = [variance_inflation_factor(c, ix) for ix in np.arange(c.shape[1])]\n",
    "        maxloc = vif.index(max(vif))\n",
    "    # Drop variables with VIF values greater than 10    \n",
    "    if max(vif) > thresh: \n",
    "        print('dropping \\'' + X[cols[variables]].columns[maxloc] + '\\' at index: ' + str(maxloc))\n",
    "        variables = np.delete(variables, maxloc)\n",
    "        dropped=True\n",
    "        \n",
    "        print('Remaining variables:')\n",
    "        print(X.columns[variables])\n",
    "    return X.iloc[:, variables]\n",
    "\n",
    "x_train_selected_df=calculate_vif_(x_train_selected_df, 10.0)\n",
    "        \n",
    "x_train_selected_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate VIF for all independent variables and drop variables with VIF > 10\n",
    "thresh=10\n",
    "\n",
    "X = x_train_selected_df\n",
    "\n",
    "dropped=True\n",
    "\n",
    "while dropped==True:\n",
    "    cols = X.columns\n",
    "    variables = np.arange(X.shape[1])\n",
    "    \n",
    "    while dropped:\n",
    "        dropped=False\n",
    "        c = X[cols[variables]].values\n",
    "        vif = [variance_inflation_factor(c, ix) for ix in np.arange(c.shape[1])]\n",
    "        maxloc = vif.index(max(vif))\n",
    "\n",
    "    # Drop variables with VIF values greater than 10    \n",
    "    if max(vif) > thresh: \n",
    "        print('dropping \\'' + X[cols[variables]].columns[maxloc] + '\\' at index: ' + str(maxloc))\n",
    "        variables = np.delete(variables, maxloc)\n",
    "        dropped=True\n",
    "        \n",
    "        print('Remaining Shape:')\n",
    "        print(X.shape)\n",
    "    else:\n",
    "        dropped=False\n",
    "    X = X.iloc[:, variables]\n",
    "\n",
    "x_train_selected_df = X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the step above, repeatedly, until no more variables are dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" reducing the test set to have the same columns as the training set \"\"\"\n",
    "\n",
    "x_test_selected_df=x_test_selected_df[x_train_selected_df.columns]\n",
    "print('Remaining variables in test set:')\n",
    "print(x_test_selected_df.columns)\n",
    "        \n",
    "x_test_selected_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the statsmodels package to see \"friendlier\" looking Logistic output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" for the statsmodels, need to add a constant to fit intercept \"\"\"\n",
    "\n",
    "x_train_selected_df = sm.add_constant(x_train_selected_df, has_constant='add')\n",
    "x_test_selected_df  = sm.add_constant(x_test_selected_df, has_constant='add')\n",
    "\n",
    "\"\"\" modeling with selected variables from steps above \"\"\"\n",
    "\n",
    "model = sm.Logit(y_train_df, x_train_selected_df)\n",
    "result = model.fit(maxiter=100)\n",
    "\n",
    "print(\" \") \n",
    "print(\"Logistic model with selected variables\")\n",
    "print(\" \") \n",
    "print(result.summary2())\n",
    "print('AIC: ', result.aic)\n",
    "print('BIC: ', result.bic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the statsmodels Logit model to score the data and evaluate goodness-of-fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" predicted values using the model \"\"\"\n",
    "\"\"\" using numpy copy to store result as an array \"\"\"\n",
    "\"\"\" manually adding a column header \"\"\"\n",
    "predict_train = pd.DataFrame(np.copy(result.predict(x_train_selected_df)), columns=[\"P_churn_year_plus1_ind\"])\n",
    "predict_test = pd.DataFrame(np.copy(result.predict(x_test_selected_df)), columns=[\"P_churn_year_plus1_ind\"])\n",
    "\n",
    "print ('AROC train: ', metrics.roc_auc_score(y_train_df, predict_train))\n",
    "print ('AROC test: ', metrics.roc_auc_score(y_test_df, predict_test))\n",
    "\n",
    "\"\"\" note that the index for the predicted values vector \"\"\"\n",
    "\"\"\" is reset, and thus complicates a join back with the original \"\"\"\n",
    "\"\"\" thus, reset the index on the original data, then join with predicted \"\"\"\n",
    "\"\"\" WARNING: you have to do the same, to join back with dataframe of regressors \"\"\"\n",
    "y_train_df = y_train_df.reset_index(drop=True)\n",
    "y_test_df = y_test_df.reset_index(drop=True)\n",
    "\n",
    "\"\"\" joining predicted and actuals \"\"\"\n",
    "\"\"\" NOTE:  for reference, for now...to be used more later to create complete \"\"\"\n",
    "\"\"\"     dataframe with all obs and inputs and outputs \"\"\"\n",
    "train_accounts = train_acct_IDs_df.reset_index(drop=True)\n",
    "pred_v_actual_train = y_train_df.join(predict_train)\n",
    "pred_v_actual_train = pred_v_actual_train.join(train_accounts)\n",
    "\n",
    "test_accounts = test_acct_IDs_df.reset_index(drop=True)\n",
    "pred_v_actual_test = y_test_df.join(predict_test)\n",
    "pred_v_actual_test = pred_v_actual_test.join(test_accounts)\n",
    "\n",
    "print(\" \") \n",
    "print(\"MSE train:\", mean_squared_error(y_train_df, predict_train))\n",
    "print(\"MSE test:\", mean_squared_error(y_test_df, predict_test))\n",
    "\n",
    "print(\" \") \n",
    "print(\"testing that the joining process works. if so, these MSE should match the ones above.\")\n",
    "print(\"mean squared error train:\" , (np.mean((pred_v_actual_train['churn_year_plus1_ind']-pred_v_actual_train['P_churn_year_plus1_ind'])**2)))\n",
    "print(\"mean squared error testn:\" , (np.mean((pred_v_actual_test['churn_year_plus1_ind']-pred_v_actual_test['P_churn_year_plus1_ind'])**2)))\n",
    "\n",
    "pred_v_actual_test.boxplot(column='P_churn_year_plus1_ind', by='churn_year_plus1_ind',figsize=(15,15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now to try a more advanced Machine Learning algorithm to see if we can get a better model:\n",
    "**NOTE** - This cell might take some time to run depending on number of estimators \n",
    "\n",
    "Make function for scoring model\n",
    "\n",
    "Include optional *fullstats* parameter which determines if full stats and plots are generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_model(my_model, y_test_df, y_train_df, fullstats=True):\n",
    "    \"\"\" returns overall accuracy\"\"\"\n",
    "    accuracy = my_model.score(x_test_selected_df, y_test_df)\n",
    "    print(\"GB overall accuracy:\", accuracy)\n",
    "\n",
    "    \"\"\" predicted values using the model \"\"\"\n",
    "    \"\"\" using numpy copy to store result as an array \"\"\"\n",
    "    \"\"\" manually adding a column header \"\"\"\n",
    "    predict_train = pd.DataFrame(np.copy(my_model.predict_proba(x_train_selected_df)[:, 1]), columns=[\"P_churn_year_plus1_ind\"])\n",
    "    predict_test = pd.DataFrame(np.copy(my_model.predict_proba(x_test_selected_df)[:, 1]), columns=[\"P_churn_year_plus1_ind\"])\n",
    "\n",
    "    print ('AROC train: ', metrics.roc_auc_score(y_train_df, predict_train))\n",
    "    print ('AROC test: ', metrics.roc_auc_score(y_test_df, predict_test))\n",
    "\n",
    "    if fullstats:\n",
    "\n",
    "        \"\"\" note that the index for the predicted values vector \"\"\"\n",
    "        \"\"\" is reset, and thus complicates a join back with the original \"\"\"\n",
    "        \"\"\" thus, reset the index on the original data, then join with predicted \"\"\"\n",
    "        \"\"\" WARNING: you have to do the same, to join back with dataframe of regressors \"\"\"\n",
    "        y_train_df = y_train_df.reset_index(drop=True)\n",
    "        y_test_df = y_test_df.reset_index(drop=True)\n",
    "\n",
    "        train_accounts = train_acct_IDs_df.reset_index(drop=True)\n",
    "        pred_v_actual_train = y_train_df.join(predict_train)\n",
    "        pred_v_actual_train = pred_v_actual_train.join(train_accounts)\n",
    "\n",
    "\n",
    "        test_accounts = test_acct_IDs_df.reset_index(drop=True)\n",
    "        pred_v_actual_test = y_test_df.join(predict_test)\n",
    "        pred_v_actual_test = pred_v_actual_test.join(test_accounts)\n",
    "\n",
    "        print(\" \") \n",
    "        print(\"MSE train:\", mean_squared_error(y_train_df, predict_train))\n",
    "        print(\"MSE test:\", mean_squared_error(y_test_df, predict_test))\n",
    "\n",
    "        print(\" \") \n",
    "        print(\"testing that the joining process works. if so, these MSE should match the ones above.\")\n",
    "        print(\"mean squared error train:\" , (np.mean((pred_v_actual_train['churn_year_plus1_ind']-pred_v_actual_train['P_churn_year_plus1_ind'])**2)))\n",
    "        print(\"mean squared error test:\" , (np.mean((pred_v_actual_test['churn_year_plus1_ind']-pred_v_actual_test['P_churn_year_plus1_ind'])**2)))\n",
    "\n",
    "\n",
    "        pred_v_actual_test.boxplot(column='P_churn_year_plus1_ind', by='churn_year_plus1_ind',figsize=(15,15))\n",
    "\n",
    "        \"\"\" plotting AROC with each iteration of the Gradient Boosting algorithm \"\"\"\n",
    "\n",
    "        \"\"\" converting Y dataframes into arrays as needed for logic below \"\"\"\n",
    "        y_traint_array = y_train_df.values\n",
    "        y_test_array = y_test_df.values\n",
    "\n",
    "        test_AROC  = np.zeros((params['n_estimators'],), dtype=np.float64)\n",
    "        train_AROC = np.zeros((params['n_estimators'],), dtype=np.float64)\n",
    "\n",
    "        for i, y_pred in enumerate(my_model.staged_predict_proba(x_test_selected_df)):\n",
    "            test_AROC[i] = metrics.roc_auc_score(y_test_array, y_pred[:, 1])\n",
    "\n",
    "        for i, y_pred in enumerate(my_model.staged_predict_proba(x_train_selected_df)):\n",
    "            train_AROC[i] = metrics.roc_auc_score(y_train_array, y_pred[:, 1])\n",
    "\n",
    "        plt.figure(figsize=(15,15))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.title('AROC by iteration')\n",
    "        plt.plot(np.arange(params['n_estimators']) + 1, train_AROC, 'b-',\n",
    "                label='Training Set AROC')\n",
    "        plt.plot(np.arange(params['n_estimators']) + 1, test_AROC, 'r-',\n",
    "                label='Test Set AROC')\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.xlabel('Boosting Iterations')\n",
    "        plt.ylabel('AROC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting Model 1 (model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Gradient Boosting \"\"\"\n",
    "params = {'n_estimators': 50, 'min_samples_split': 2,  'max_depth': 5, 'learning_rate': 0.01}\n",
    "model2 = GradientBoostingClassifier(**params)\n",
    "\n",
    "\"\"\" had to switch the order of X and Y for the fit to work with GradientBoosting \"\"\"\n",
    "result2 = model2.fit(x_train_selected_df, y_train_array)\n",
    "\n",
    "score_model(result2, y_test_df, y_train_df, fullstats=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting Model 2 (model3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Gradient Boosting \"\"\"\n",
    "params = {'n_estimators': 250, 'min_samples_split': 2,  'max_depth': 5, 'learning_rate': 0.01}\n",
    "model3 = GradientBoostingClassifier(**params)\n",
    "\n",
    "\"\"\" had to switch the order of X and Y for the fit to work with GradientBoosting \"\"\"\n",
    "result3 = model3.fit(x_train_selected_df, y_train_array)\n",
    "\n",
    "score_model(result3, y_test_df, y_train_df, fullstats=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting Model 3 (model4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Gradient Boosting \"\"\"\n",
    "params = {'n_estimators': 50, 'min_samples_split': 2,  'max_depth': 5, 'learning_rate': 0.1}\n",
    "model4 = GradientBoostingClassifier(**params)\n",
    "\n",
    "\"\"\" had to switch the order of X and Y for the fit to work with GradientBoosting \"\"\"\n",
    "result4 = model4.fit(x_train_selected_df, y_train_array)\n",
    "\n",
    "score_model(result4, y_test_df, y_train_df, fullstats=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Champion Churn Model\n",
    "The gradient boosting model with 0.01 learning rate and n_estimators=50 will be chosen as champion because it had best performance while also being fastest in compute time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_model(result2, y_test_df, y_train_df, fullstats=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance for the Gradient Boosting Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = model2.feature_importances_\n",
    "\"\"\" make importances relative to max importance \"\"\"\n",
    "feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "plt.figure(figsize=(50,50))\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.barh(pos, feature_importance[sorted_idx], align='center')\n",
    "plt.yticks(pos, x_train_selected_df.columns[sorted_idx])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.title('Variable Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Append the train and test sets, and keeping the acct_IDs and predicted churn probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" append together train and test predicted values, keeping only acct ID and predicted values \"\"\"\n",
    "\n",
    "pred_v_actual_train = pd.concat([pred_v_actual_train, pred_v_actual_test], ignore_index=True)\n",
    "\n",
    "pred_v_actual_train = pred_v_actual_train.drop(['churn_year_plus1_ind'],axis=1)\n",
    "\n",
    "\"\"\" and then save predicted values by acct ID to csv \"\"\"\n",
    "pred_v_actual_train.to_csv('churn_scores.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
